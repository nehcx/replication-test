"""Search lexical variale in poetic Japanese langage.

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oVURf2H7T71bBVebr9zSFwB_QrH7U1mh
"""

import json
import numpy as np
import pandas as pd
from logging import basicConfig, getLogger, DEBUG
from collections import Counter, OrderedDict, defaultdict
from itertools import chain
from tqdm import tqdm
from omegaconf.dictconfig import DictConfig
from dataclasses import dataclass
from heapq import nlargest
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import ClusterWarning
from warnings import simplefilter
simplefilter("ignore", ClusterWarning)

basicConfig(format="%(asctime)s %(message)s", level=DEBUG)
logger = getLogger(__name__)


def load_data(corpus_f, context_f, id2lemma_f, top_n_target, top_n_feature,
              only_content_feature):
    """Load processed data.

    :param corpus_f: csv, path of corpus file
    :param context_f: json, path of context words semantic change degree
    :param id2lemma: json, path of file for transforming metacode to lemma
    :param top_n_target: int, top n frequent types as targets
    :param top_n_feature: int, top n semantically less changed context words

    :return feature: list, list of context feature words
    :return hd: pandas dataframe, hachidaishu text corpus
    :return id2lemma: dict, metacode to lemma dictionary
    :return target_type_lst: list, list of target types
    """
    with open(id2lemma_f) as f_1, open(context_f) as f_2:
        id2lemma = json.load(f_1)
        context = json.load(f_2)
    stop_lst = ["03", "04", "05", "07", "08", "09", "16"]
    type_lst = [x for x in id2lemma.keys() if x.split("-")[1] not in stop_lst]
    if only_content_feature:
        context = {k: v for k, v in context.items() if k in type_lst}
    feature = list(context.keys())[:top_n_feature]
    hd = pd.read_csv(corpus_f)
    hd["cleaned"] = hd.source.map(
        lambda x: [x for x in x.split(",") if x in type_lst])
    print("numbers of tokens: %s" % sum(hd.source.str.split(",").map(len)))
    print("numbers of types: %s" % len(list(chain(hd.source.str.split(",")))))
    print("numbers of tokens after cleaning: %s" % sum(hd.cleaned.map(len)))
    print("numbers of types after cleaning: %s" % len(type_lst))
    token_lst = []
    for poem in hd.cleaned:
        token_lst += poem
    type_freq_dic = dict(OrderedDict(Counter(token_lst).most_common()))
    target_type_lst = nlargest(top_n_target,
                               type_freq_dic,
                               key=type_freq_dic.get)
    return feature, hd, id2lemma, target_type_lst


def co_oc_mat(texts, window_size, feature):
    """Construct co-occurrence frequency matrix.

    :param texts: pandas.series, of processed texts
    :param window_size: int, window size
    :param feature: list of str, word list used as context feature

    :return mat: pandas.dataframe, type-type co-occurence frequency matrix
    """
    count_d = defaultdict(int)
    types = set()
    for text in texts:
        for i in range(len(text)):
            token = text[i]
            types.add(token)
            next_token = text[i + 1:i + 1 + window_size]
            for t in next_token:
                key = tuple(sorted([t, token]))
                count_d[key] += 1
    types = sorted(types)
    mat = pd.DataFrame(data=np.zeros((len(feature), len(types)),
                                     dtype=np.int16),
                       index=feature,
                       columns=types)
    pbar = tqdm(count_d.items())
    for key, value in pbar:
        # pbar.set_description(f"writing {str(key)}")
        pbar.set_description("writing")
        mat.at[key[0], key[1]] = value
        mat.at[key[1], key[0]] = value
    return mat


def ppmi_mat(freq_mat, feature, target_lst):
    """Obtain ppmi matrix.

    :param freq_mat: pandas.dataframe, type-type co-occurence frequency matrix
    :param feature: list, list of context words as feature
    :param target_lst: list, list of target types or committee

    :return mat: pandas.dataframe, type-feature ppmi matrix
    """
    freq_mat = freq_mat.copy()
    col_totals = freq_mat.sum(axis=0)
    total = col_totals.sum()
    row_totals = freq_mat.sum(axis=1)
    expected = np.outer(row_totals, col_totals) / total
    mat = freq_mat / expected
    with np.errstate(divide="ignore"):
        mat = np.log2(mat)
    mat[np.isinf(mat)] = 0.0  # log(0) = 0
    mat[np.isnan(mat)] = 0.0
    mat[mat < 0] = 0.0
    mat = mat.loc[mat.index.isin(feature), target_lst]
    return mat


def _cosin_sim(v_i, v_j):
    """Compute consine similarity of two vevtors."""
    return np.dot(v_i, v_j) / (np.linalg.norm(v_i) * np.linalg.norm(v_j))


def cos_mat(ppmi_mat, target_type_lst):
    """Obtain cosine similarity matrix.

    :param ppmi_mat: pandas.dataframe: ppmi matrix
    :param target_type_lst: target types

    :return mat: pandas.dataframe, type-type matrix
    """
    sim_d = defaultdict()
    t_set = sorted(set(target_type_lst))
    pbar = tqdm(range(len(target_type_lst)))
    for i in pbar:
        pbar.set_description("computing similarity")
        t_i = target_type_lst[i]
        next_t = target_type_lst[i:]
        for t_j in next_t:
            key = tuple(sorted([t_i, t_j]))
            # pbar.set_description(f"computing similarity {key}")
            v_t_i = np.array(ppmi_mat[t_i])
            v_t_j = np.array(ppmi_mat[t_j])
            sim_d[key] = _cosin_sim(v_t_i, v_t_j)
    mat = pd.DataFrame(data=np.zeros((len(t_set), len(t_set)),
                                     dtype=np.float64),
                       index=t_set,
                       columns=t_set)
    pbar = tqdm(sim_d.items())
    for key, value in pbar:
        # pbar.set_description(f"writing similarity {key}")
        pbar.set_description(f"writing similarity")
        mat.at[key[0], key[1]] = value
        mat.at[key[1], key[0]] = value
    return mat


def _cluster(cos_mat, l, k, alpha):
    """Hierarchical agglomerative clustering with average linkage."""
    top_n_l = cos_mat.nlargest(k, l).index.tolist()
    if len(top_n_l) == 1:
        c_lst = [tuple(top_n_l)]
    else:
        sub_CosM = np.array(cos_mat.loc[top_n_l, top_n_l])
        clusters = AgglomerativeClustering(n_clusters=None,
                                           distance_threshold=alpha,
                                           affinity="cosine",
                                           linkage="average").fit(sub_CosM)
        labels = list(clusters.labels_)
        labeled_lemma = list(zip(top_n_l, labels))
        c_dic = {}
        for label in set(labels):
            c_dic[label] = tuple()
            for lemma in labeled_lemma:
                if lemma[1] == label:
                    c_dic[label] += (lemma[0], )
                    c_dic[label] = tuple(sorted(c_dic[label]))
        c_lst = list(c_dic.values())
    return c_lst


def committee(target_type_lst, cos_mat, k, alpha):
    """Extract committee of types.

    :param target_type_lst: list, list of target types
    :param cos_mat: pandas.dataframe, type-type similarity matrix
    :param k: int, top-k similar elements of l
    :param alpha: float, cutting height of dendrogram

    :return c_lst: list, list of committee
    """
    c_lst = []
    for l in target_type_lst:
        c_lst += _cluster(cos_mat.loc[target_type_lst, target_type_lst], l, k,
                          alpha)
    return c_lst


def _avg_sim(cos_mat, c):
    """Calculate pairwise average similarity of types in a committee."""
    s = 0
    n_pair = 0
    for i in range(len(c)):
        if len(c) == 1:
            avg_s = 1
        else:
            c_i = c[i]
            next_c = c[i + 1:]
            for c_j in next_c:
                n_pair += 1
                s += cos_mat.loc[c_i, c_j]
            if n_pair != 0:
                avg_s = s / n_pair
    return avg_s


def filter_sort(c_lst, cos_mat):
    """Filter and sort committees.

    :param c_lst: list of tuple, list of committees
    :param cos_mat: pandas.dataframe, type-type similarity matrix

    :return c_lst_cleaned, list of tuple, filetered sorted committee list
    """
    # remove dublicate c
    c_lst = list(set(c_lst))
    rm_lst = set()
    for i in range(len(c_lst)):
        c_i = c_lst[i]
        next_c = c_lst[:i] + c_lst[i + 1:]
        for c_j in next_c:
            if len(c_i) == 1:
                # remove singleton included in other larger c
                if set(c_i).issubset(c_j):
                    rm_lst.add(c_i)
            else:
                # remove larger c if c included other smaller c
                if set(c_i).issubset(c_j):
                    rm_lst.add(c_j)
    # removing
    for c in rm_lst:
        c_lst.remove(c)
    # store average similarity of each committee
    c_sim_dic = {}
    for c in c_lst:
        c_sim_dic[c] = _avg_sim(cos_mat, c)
    c_lst_cleaned = list(zip(c_sim_dic.keys(), c_sim_dic.values()))
    # sort
    c_lst_cleaned = sorted(c_lst_cleaned, key=lambda x: x[1])

    return c_lst_cleaned


def merge_1(c_lst, freq_mat, feature):
    """Merge part 1.

    :param c_lst: list of tuple, list of committees with similarity
    :param freq_mat: pandas.dataframe, type-tyoe frequency matrix
    :param feature: list of str, list of context words as feature

    :return c_freq_mat: pandas.dataframe, committee-type frequency matrix
    :return c_ppmi_mat: pandas.dataframe, committee-feature ppmi matrix
    :return c_cos_mat: pandas.dataframe, committee-commitee cosine similarity
                       matrix
    """
    c_lst = list(map(lambda x: ",".join(x), list(list(zip(*c_lst))[0])))
    c_vec_dic = {}
    pbar = tqdm(c_lst)
    for c in pbar:
        # pbar.set_description(f"average frequency vec {c}")
        avg_freq_vec = freq_mat[c.split(",")].mean(axis=1)
        c_vec_dic[c] = list(avg_freq_vec)

    c_freq_mat = pd.DataFrame(c_vec_dic)
    c_freq_mat.index = freq_mat.index
    c_ppmi_mat = ppmi_mat(c_freq_mat, feature, c_lst)
    c_ppmi_mat = c_ppmi_mat.loc[c_ppmi_mat.index.isin(feature)]
    c_cos_mat = cos_mat(c_ppmi_mat, list(c_ppmi_mat.columns))
    return c_freq_mat, c_ppmi_mat, c_cos_mat


def merge_2(c_cos_mat, l_cos_mat, beta):
    """Merge part 2.

    :param c_cos_mat: pandas.dataframe, committee cosine similarity matrix
    :param l_cos_mat: pandas.dataframe, lemama (type) cosine similarity matrix
    :param beta: threshold to judge whether to merge two committees

    :return new_c_lst_with_sim: list of tuple, list of newly merged committees
                                with average similarity
    """
    new_c_lst = []
    merged_lst = []
    c_lst = c_cos_mat.columns
    n_c = len(c_lst)
    pbar = tqdm(range(len(c_lst)))
    for i in pbar:
        pbar.set_description("try merging")
        target_c = c_lst[i]
        neighbor_c_lst = c_cos_mat.nlargest(n_c, target_c).index.tolist()
        n = 0
        neighbor_c = None
        while (neighbor_c in merged_lst) | (neighbor_c is None):
            # pbar.set_description(f"try merging {target_c} and {neighbor_c}")
            n += 1
            if n < len(c_cos_mat):
                neighbor_c = neighbor_c_lst[-1]
            else:
                neighbor_c = target_c
        sim = c_cos_mat.loc[target_c, neighbor_c]
        if (sim >= beta) & (neighbor_c != target_c):
            merged_lst.append(neighbor_c)
            target_c = tuple(target_c.split(","))
            neighbor_c = tuple(neighbor_c.split(","))
            new_c = tuple(target_c + neighbor_c)
            new_c = tuple(sorted(set(new_c)))
        else:
            target_c = tuple(target_c.split(","))
            new_c = target_c
        if new_c not in new_c_lst:
            new_c_lst.append(new_c)

    new_c_lst_with_sim = {}
    pbar = tqdm(new_c_lst)
    for c in pbar:
        # pbar.set_description(f"computing average similarity {c}")
        pbar.set_description("computing average similarity")
        new_c_lst_with_sim[c] = _avg_sim(l_cos_mat, c)

    new_c_lst_with_sim = list(
        zip(new_c_lst_with_sim.keys(), new_c_lst_with_sim.values()))
    new_c_lst_with_sim = sorted(new_c_lst_with_sim, key=lambda x: x[1])

    return new_c_lst_with_sim


def residuals(target_type_lst, ppmi_mat, c_ppmi_mat, gamma):
    """Obtain residual types for next loop.

    :param target_type_lst: list, list of target types
    :param ppmi_mat: pandas.dataframe, type-feature ppmi matrix
    :param c_ppmi_mat: pandas.dataframe, committee-feature ppmi matrix
    :param gamma: float, threshold to judge whether output committee

    :return res: list, list of residual types
    """
    res = []
    for l in tqdm(target_type_lst):
        v_l = np.array(ppmi_mat[l])
        sim = 0
        n = 0
        c = ""
        while (n < len(c_ppmi_mat.columns)) & (sim < gamma):
            c = c_ppmi_mat.columns[n]
            v_c = np.array(c_ppmi_mat[c])
            sim = _cosin_sim(v_l, v_c)
            n += 1
        if sim < gamma:
            res.append(l)
        else:
            pass
    print(f"remain {len(res)} lemmas to the next loop..")
    return res


def search(corpus_f, context_f, id2lemma_f, top_n_target: int,
           top_n_feature: int, only_content_feature: bool, k: int,
           alpha: float, beta: float, gamma: float, window_size: int):
    """Search lexical variable."""
    # assert gamma >= 0.5
    logger.info("[INFO] load data...")

    feature, hd, id2lemma, target_type_lst = load_data(corpus_f, context_f,
                                                       id2lemma_f,
                                                       top_n_target,
                                                       top_n_feature,
                                                       only_content_feature)

    # initial matrice as input
    logger.info(
        "[INFO] computing lemma-by-lemma co-occurence frequency matrix...")
    FreqM = co_oc_mat(hd.cleaned, window_size, feature)
    logger.info("[INFO] computing lemma-by-feature PPMI matrix...")
    ppmiM = ppmi_mat(FreqM, feature, target_type_lst)
    logger.info(
        "[INFO] computing target lemma-by-lemma cosine similarity matrix...")
    CosM = cos_mat(ppmiM, target_type_lst)

    # loop
    oldR = []
    R = target_type_lst
    n = 0
    finalC = []
    while (R != []) & (R != oldR):
        oldR = R
        n += 1
        logger.info(f"[INFO] entering loop {n}...")
        logger.info(f"[INFO] loop {n}: obtaining initial committees...")
        C = committee(R, CosM, k, alpha)
        logger.info(
            f"[INFO] loop {n}: filtering and sorting initial committees...")
        C = filter_sort(C, CosM)
        logger.info(f"[INFO] loop {n}: merging committees (part 1)...")
        CFreqM, CppmiM, ComM = merge_1(C, FreqM, feature)
        logger.info(f"[INFO] loop {n}: merging committees (part 2)...")
        newCWithSim = merge_2(ComM, CosM, beta)
        newC = sorted(newCWithSim, key=lambda x: x[1])
        logger.info(
            f"[INFO] loop {n}: repeating merging part 1 for new committees...")
        newCFreqM, newCppmiM, newComM = merge_1(newC, FreqM, feature)
        logger.info(
            f"[INFO] loop {n}: obtaining residual lemmas for next loop...")
        R = residuals(oldR, ppmiM, newCppmiM, gamma)
        logger.info(f"[INFO] loop {n}: saving obtained commitees...")
        for c_new in newC:
            c_new_checked = True
            for c_old in finalC:
                if set(c_old[0]).issubset(c_new[0]):
                    finalC.remove(c_old)
                if set(c_new[0]).issubset(c_old[0]):
                    c_new_checked = False
            if (c_new not in finalC) & (c_new_checked):
                finalC.append(c_new)
    logger.info(f"[INFO] total {n} loops; summarizing results...")
    finalC = sorted(finalC, key=lambda x: x[1])
    # summary = pd.DataFrame()
    # summary["bg_id"] = list(zip(*finalC))[0]
    # summary["lemma"] = summary.bg_id.map(
    #     lambda x: tuple([id2lemma[l][0] for l in x]))
    # summary["reading"] = summary.bg_id.map(
    #     lambda x: tuple([id2lemma[l][1] for l in x]))
    # summary["average_similarity"] = list(zip(*finalC))[1]
    # summary.to_csv(
    #     "../artifact/k-{};alpha-{};beta-{};gamma-{};window_size-{}.csv".format(
    #         k, alpha, beta, gamma, window_size),
    #     index=False)
    logger.info("[INFO] wrote results.")
    return finalC


# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=2, TOP=500)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=3, TOP=500)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=3, TOP=1000)

# search(k=50, alpha=0.5, beta=0.4, gamma=0.2, window_size=2, TOP=1000)

# search(k=50, alpha=0.1, beta=0.4, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.4, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.4, window_size=3, TOP=1000)

# search(k=100, alpha=0.5, beta=0.8, gamma=0.6, window_size=4, TOP=1000)

# main(corpus_f="../data/parsed_hd.csv",
#      context_f="../cache/id2change.json",
#      id2lemma_f="../data/id2lemma.json",
#      top_n_feature=3000,
#      top_n_target=1000,
#      only_content_feature=False,
#      window_size=2,
#      k=100,
#      alpha=0.5,
#      beta=0.8,
#      gamma=0.6)


@dataclass
class Results:
    """Results metacode matching degrees."""

    config: DictConfig

    def __post_init__(self):
        self.res_df = self._df()

    def _df(self):
        args = self.config
        logger.debug(f"[DEBUG] args: {args}")
        res = search(**self.config)
        res_df = pd.DataFrame()
        res_df["bg_id"] = list(zip(*res))[0]
        assert len(res_df.index) != 0
        # average similarity among variable
        res_df["avg_sim"] = list(zip(*res))[1]
        res_df = res_df[res_df.bg_id.map(lambda x: True
                                         if len(x) >= 2 else False)]
        with open("../data/id2lemma.json") as fn:
            id2lemma = json.load(fn)
        res_df["lemma"] = res_df.bg_id.map(
            lambda x: tuple([id2lemma[l][0] for l in x]))
        # reading
        res_df["rdg"] = res_df.bg_id.map(
            lambda x: tuple([id2lemma[l][1] for l in x]))
        # number of variants
        res_df["num"] = res_df.bg_id.apply(len)
        res_df["pos_match"], res_df["group_match"], res_df[
            "field_match"] = tuple(zip(*res_df["bg_id"].apply(self._match)))
        return res_df

    @staticmethod
    def _match(var_):
        """Check whether match at specific levels.

        :param var_: tuple of str, pseudo lexical variable

        :return pos_match: bool, whether match at pos level
        :return group_match: bool, whether match at group level
        :return field_match: bool, whether match at field level
        """
        assert len(var_) > 1
        first_l_decomp = var_[0].split("-")
        pos_match = all(l.split("-")[1] == first_l_decomp[1] for l in var_)
        group_match = all(
            l.split("-")[1:3] == first_l_decomp[1:3] for l in var_)
        field_match = all(
            l.split("-")[1:4] == first_l_decomp[1:4] for l in var_)
        return pos_match, group_match, field_match

    def write_(self):
        """Write results csv."""
        self.res_df.to_csv(
            "../artifact/fn-{}_ln-{}_oc-{}_k-{}_a-{}_b-{}_g-{}_ws-{}.csv".
            format(self.config.top_n_feature, self.config.top_n_target,
                   self.config.only_content_feature, self.config.k,
                   round(self.config.alpha, 1), round(self.config.beta, 1),
                   round(self.config.gamma, 1), self.config.window_size),
            index=False)

    @property
    def pos_match_num(self):
        """Obtain number of correct pos level matching."""
        return sum(self.res_df.pos_match.to_list())

    @property
    def group_match_num(self):
        """Obtain number of correct group level matching."""
        return sum(self.res_df.group_match.to_list())

    @property
    def field_match_num(self):
        """Obtain number of correct field level matching."""
        return sum(self.res_df.field_match.to_list())

    @property
    def pos_match_per(self):
        """Obtain percentage of correct matching at pos level."""
        return self.pos_match_num / len(self.res_df.pos_match)

    @property
    def group_match_per(self):
        """Obtain percentage of correct match at group level."""
        return self.group_match_num / len(self.res_df.group_match)

    @property
    def field_match_per(self):
        """Obtain percentage of correct match at group level."""
        return self.field_match_num / len(self.res_df.field_match)
